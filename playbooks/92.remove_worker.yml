---
- hosts: k8s
  any_errors_fatal: true
  gather_facts: false
  tasks:
    - name: check nodes
      assert:
        that:
          - nodes is defined
          - nodes|length > 0
        fail_msg: 'The extra argument "nodes" can not be empty'
      run_once: true
      delegate_to: localhost

    - name: get all worker nodes in the cluster
      shell: >-
        set -o pipefail &&
        kubectl get node
        -l 'node-role.kubernetes.io/worker'
        --no-headers |
        awk '{print $1}'
      register: result
      run_once: true
      delegate_to: "{{ groups.k8s_master.0 }}"

    - set_fact:
        workers: "{{ result.stdout_lines }}"
      run_once: true
      delegate_to: "{{ groups.k8s_master.0 }}"

- hosts: k8s_master[0]
  any_errors_fatal: true
  gather_facts: false
  tasks:
    - include_role:
        name: k8s_worker/remove
      vars:
        worker: "{{ item }}"
      when: item in workers
      with_items: "{{ nodes|split(',')|unique }}"

- hosts: "{{ nodes }}"
  any_errors_fatal: true
  tasks:
    - block:
        - name: populate service facts
          service_facts:
        - include_tasks: roles/reset/tasks/kubelet.yml
      when: inventory_hostname in workers

- hosts: k8s
  any_errors_fatal: true
  gather_facts: false
  tasks:
    - block:
        - name: remove node hosts information
          blockinfile:
            path: /etc/hosts
            state: absent
            marker: "# {mark} ANSIBLE MANAGED BLOCK {{ item }}"
          when: item in workers
          with_items: "{{ nodes|split(',')|unique }}"
      when: inventory_hostname not in nodes|split(',')|unique

# 集群节点有增减，重新执行安装命令以更新 prometheus 监控主机配置
- hosts: k8s_master[0]
  any_errors_fatal: true
  gather_facts: false
  roles:
    - role: k8s_addon/prometheus
      vars:
        remove_worker: true
      when: PROMETHEUS_ENABLE == "yes"
